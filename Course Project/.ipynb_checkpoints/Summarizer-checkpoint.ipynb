{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Content\", \"Important\"]\n",
    "train = pd.concat([\n",
    "    pd.read_excel(\"etools/emails_300_set_1.xlsx\", header=1)[columns],\n",
    "    pd.read_excel(\"etools/emails_300_set_2.xlsx\", header=1)[columns],\n",
    "    pd.read_excel(\"etools/emails_300_set_3.xlsx\", header=1)[columns],  \n",
    "])\n",
    "\n",
    "train.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etools.ranker as erank\n",
    "import etools.summarizer as esum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "importlib.reload(erank)\n",
    "importlib.reload(esum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([erank.score(email) for email in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "for i, email in enumerate(test):\n",
    "    a = erank.is_important(email)\n",
    "    if not a:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.) a. I have made most of the changes suggested in Gayle's fax of 11/29 except that the definition of Performance Assurance needs to stay as it is given that it does not only mean the Escrowed Funds (for instance, in the event of Downgrade Event with respect to Enron Corp., EPMI would have to post Performance Assurance).\n",
      "2.) We have also not amended the last line of Section 5.5 since, among other things, the two amendments taken together would mean that in the event of a dispute, the Non-Defaulting Party would be limited to receiving only what is in the Escrow Fund.\n",
      "1.) I will be happy to incorporate the Escrow language in the La Salle Bank Escrow Agreement once someone from Black Hills lets me know that the Escrow language is acceptable as written.\n",
      "2.) a. I have made most of the changes suggested in Gayle's fax of 11/29 except that the definition of Performance Assurance needs to stay as it is given that it does not only mean the Escrowed Funds (for instance, in the event of Downgrade Event with respect to Enron Corp., EPMI would have to post Performance Assurance).\n"
     ]
    }
   ],
   "source": [
    "email = test[1]\n",
    "for i, s in enumerate(esum.significant_sentences(email)):\n",
    "    print(\"{}.)\".format(i + 1), s)\n",
    "    \n",
    "for i, s in enumerate(esum.summarizing_sentences(email)):\n",
    "    print(\"{}.)\".format(i + 1), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Word Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load in the training and test data.\n",
    "train = pd.read_excel(\"../Data/emails_25000.xlsx\", header=1)[\"Content\"]\n",
    "test  = pd.concat([\n",
    "    pd.read_excel(\"../Data/emails_300_set_1.xlsx\", header=1),\n",
    "    pd.read_excel(\"../Data/emails_300_set_2.xlsx\", header=1),\n",
    "    pd.read_excel(\"../Data/emails_300_set_3.xlsx\", header=1),\n",
    "]).reset_index()[\"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(document: str) -> str:\n",
    "    document = document.lower()\n",
    "    words = \"\".join(c for c in document if c.isalpha() or c.isspace()).split()\n",
    "    return \" \".join(w for w in words if w not in ENGLISH_STOP_WORDS)\n",
    "\n",
    "clean(\"Hello. My name is Daniel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a TF-IDF Vectorizer.\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    token_pattern=\"[A-Za-z]{3,}\"  # Letter-only words of length at least 3.\n",
    ").fit(clean(email) for email in train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_words(doc: str, vectorizer, num_words: int = 5):\n",
    "    \"\"\" Get the *num_words* most significant words from this *doc*. \"\"\"\n",
    "    doc = clean(doc)\n",
    "    vector = vectorizer.transform([doc]).toarray().flatten()\n",
    "    words = sorted(list(zip(vector, vectorizer.get_feature_names())))\n",
    "    try:\n",
    "        return [word for _, word in words[-num_words:]][::-1]\n",
    "    except IndexError as error:\n",
    "        print(\"Error: num_words parameter greater than number of words.\")\n",
    "        return []\n",
    "    \n",
    "get_significant_words(test[20], tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_sentence(doc: str, vectorizer, num_sentences: int = 1):\n",
    "    sentences = [sentence.string.strip() for sentence in nlp(doc).sents]\n",
    "    scores = [vectorizer.transform([clean(s)]).sum() for s in sentences]\n",
    "    pairs = [(scores[i], sentences[i]) for i in range(len(sentences))]\n",
    "    pairs = sorted(pairs)[::-1]\n",
    "    try:\n",
    "        return [pair[1] for pair in pairs[:num_sentences]]\n",
    "    except IndexError as error:\n",
    "        print(\"Error: num_words parameter greater than number of words.\")\n",
    "        return []   \n",
    "    \n",
    "email = test[20]\n",
    "sentences = get_significant_sentence(email, tfidf, num_sentences=2)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(\"{}.) \".format(i + 1), \" \".join(sentence.split()))    \n",
    "print(\"\\n=====================================\\n\")\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarizing_sentence(doc: str, num_sentences: int = 1):\n",
    "    counts = {}\n",
    "    for word in nlp(clean(doc)):\n",
    "        counts[word.lemma_] = counts.get(word.lemma_, -1) + 1\n",
    "    \n",
    "    text = nlp(doc)\n",
    "    sentences = [sentence.string.strip() for sentence in text.sents]\n",
    "    scores = [0 for _ in sentences]\n",
    "    for i, sentence in enumerate(text.sents):\n",
    "        for word in nlp(clean(sentence.string)):\n",
    "            scores[i] += counts.get(word.lemma_, 0)\n",
    "                \n",
    "    pairs = [(scores[i], sentences[i]) for i in range(len(sentences))]\n",
    "    pairs = sorted(pairs)[::-1]\n",
    "    \n",
    "    try:\n",
    "        return [pair[1] for pair in pairs[:num_sentences]]\n",
    "    except IndexError as error:\n",
    "        print(\"Error: num_words parameter greater than number of words.\")\n",
    "        return []  \n",
    "    \n",
    "email = test[20]\n",
    "sentences = get_summarizing_sentence(email, num_sentences=2)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(\"{}.) \".format(i + 1), \" \".join(sentence.split()))    \n",
    "print(\"\\n=====================================\\n\")\n",
    "print(email)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
